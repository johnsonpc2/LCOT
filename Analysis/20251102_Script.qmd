---
title: "Learning Contingencies Over Time"
author: "Pierce C. Johnson"
format: html
editor: visual
editor_options: 
  chunk_output_type: console
---

#  Setup

First I setup my environment and save information about my data files. I'll also
load in my data and save it into a single data.table.

```{r}
#| label: Setup and Data Processing
#| echo: true
#| message: false
#| warning: false

# Clear console and plot viewer
cat("\014")
graphics.off()
rm(list = ls(all.names = TRUE))

# Load packages
c("beepr", "data.table", "ggplot2", "here", "pcjtools") |>
  purrr::map(.f = library, character.only = T) |>
  invisible()

# Store the working directory
wd <- here()

# Store info about the data files
files_info(path = paste0(wd, "/data")) -> files

# Read all the data files in
lapply(
  X = files$filepath,
  FUN = fread
) -> data_list

# And bind them all into a single data.table
rbindlist(data_list, fill = TRUE) -> dt

# Count the number of subjects whose data we've collected
length(table(dt$sona_id)) -> n_subjects_initial

# Subset data
cols_to_keep <- c("trial_index", "sona_id", "phase",
                  "rt", "stimulus", "response", "block",
                  "contingency", "intact_first", "pair_type")

dt[, ..cols_to_keep] -> dt_sub

# Process data
dt_sub[(phase == "memory_trial_pair_1" |
          phase == "memory_trial_pair_2") &
          rt != "null",
       ][, `:=`(
         # Re-format the RTs to seconds
         rt = as.numeric(rt) / 1000,
         # Quantify a confidence metric
         confidence = fcase(
           intact_first == TRUE,
           1 - (as.numeric(response)) / 100,
           intact_first == FALSE,
           (as.numeric(response)) / 100,
           default = NA
         )
       )
       ][, `:=`(
         # Compute the average confidence
         avg_confidence = mean(confidence, na.rm = TRUE),
         # And conduct a t-test to find out if that subject had confidence statistically different from chance
         p_value = t.test(
           x = confidence,
           mu = 0.5,
           alternative = "greater")$p.value,
         # Compute the average RT of each subject
         avg_rt = mean(rt, na.rm = TRUE)
       ), by = sona_id
       ][avg_rt < 20,
       ] -> dt_results

# Finally, count the number of subjects retained after exclusion
length(table(dt_results$sona_id)) -> n_subjects_retained
```

Initially, we've collected data from `r as.character(n_subjects_initial)`
subjects. After exclusions, we retain data from
`r as.character(n_subjects_retained)` participants
(`r as.character(round((n_subjects_retained / n_subjects_initial) * 100), 2)`%)
who had average response times \< 20 seconds.

Now plot the average confidence rating for each contingency condition across the
three learning and test blocks:

```{r}
#| echo: false
#| message: false
#| warning: false

# Block on x-axis, contingency as separate lines
ggplot(
  data = dt_results,
  mapping = aes(
    x = factor(block),
    y = confidence,
    color = factor(contingency),
    group = factor(contingency)
    )
  ) +
  scale_x_discrete(breaks = seq(1, 3, by = 1)) +
  scale_y_continuous(limits = c(0, 1)) +
  stat_summary(
    fun.data = mean_cl_boot,
    mapping = aes(
      shape = factor(
        x = dt_results$contingency,
        levels = c("100", "75", "50"),
        labels = c("100%", "75%", "50%"))
    ),
    geom = "pointrange",
    alpha = .5
    ) +
  stat_summary(
    fun = mean,
    geom = "line",
    alpha = .5
    ) +
  guides(color = "none") +
  labs(
    shape = "Contingency",
    y = "Confidence",
    x = "Block"
  ) +
  theme(
    legend.position = c(.5, .90),
    legend.direction = "horizontal",
    legend.background = element_rect(fill = "white")
    ) -> learning_by_block

print(learning_by_block)
```
